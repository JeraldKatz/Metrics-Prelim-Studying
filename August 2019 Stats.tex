\section{2019 Stats August Answers}
\subsection{1.a}
\(Y \sim U[0,X]\), so the conditional PDF of Y: \(f_{Y|X=x}(y)=\frac{1}{x}\).
\begin{align*}
    E(Y|X=x) = \int_0^x y \cdot f_{Y|X=x}(y) dy \\
    =\int_0^x y \cdot \frac{1}{x} \;dy = \int_0^x \frac{y}{x}\; dy \\
    = \frac{y^2}{2} x^{-1} \bigg\rvert_0^x  = \frac{x^2 x^{-1}}{2} - 0 = \frac{x}{2} \\
     \boxed{E(Y|X=x) = \frac{x}{2}}
\end{align*}
Using the law of iterated expectations, we have:
\[
E[Y] = E[E(Y|X=x)] = \int_0^1 \frac{x}{2} dx = \frac{x^2}{4} \bigg\rvert_0^1
\]
\[
\boxed{E[Y] = \frac{1}{4}}
\]
To find the CDF and marginal PDF, we start with:
\[
    F_Y(y) = \int P(Y\leq y|X=x) f_x(x) dx
\] Where:
\[
P(Y\leq y|X=x) = \int_{-\infty}^yf_{Y|X=x}(t)dt
\]
Because \(y\sim U[0,x]\), we have:
\begin{gather*}
P(Y\leq y|X=x) = \int_0^y \frac{1}{x} dt = \frac{y}{x} \\
\therefore F_Y(y) = \frac{y}{x} \cdot 1 dx
\end{gather*}
Because \(x \in [y,1]\), so we have:
\begin{align*}
    F_Y(y) = \int_y^1 \frac{y}{x} dx = y \bigg ( \ln(x) \bigg\rvert_y^1 \bigg ) = y(0-\ln(y)) \\
    \therefore \boxed{F_Y(y) = -y\ln(y)}
\end{align*}
To find the marginal PDF, we take the derivative of the CDF:
\[
f_Y(y) = \frac{\partial}{\partial y}(-y\ln(y)) = -1 \left ( \frac{y}{y} + \ln(y) \right ) = \boxed{-1-\ln(y)}
\]
\subsection{1.b}
I want to show that \(E(X|Y) = E(E(X|Y,Z)|Y)\). 
Let \(\sigma(Y)\) be the smallest $\sigma$-algebra generated by the set $Y$.  Let \(\sigma(Y,Z)\) be the smallest $\sigma$-algebra generated by the set $\{Y,Z\}$.  By definition, \(\sigma(Y) \subseteq \sigma(Y,Z)\).
\\
Consider \(E(X|Y,Z)\).  Using the orthogonality property of conditional expectation, we have:
\begin{gather}
    E[(X-E(X|Y,Z))g(Y,Z)]=0
\end{gather}
where \(g(Y,Z)\) is any function that is \(\sigma(Y,Z)\)-measurable.
Now consider \(E(E(X|Y,Z)|Y)\).  By orthogonality, we have:
\begin{gather*}
    E[(E(X|Y,Z)-E(E(X|Y,Z))h(Y)] =0
\end{gather*}
Where \(h(Y)\) is any function that is \(\sigma(Y)\)-measurable.  Note that because \(h(Y)\) is \(\sigma(Y)\)-measurable, and \(\sigma(Y) \subseteq \sigma(Y,Z)\), \(h(Y)\) is \(\sigma(Y,Z)\)-measurable.  In other words, \(h(Y)\) is a special case of \(g(Y,Z)\).
Adding and subtracting $X$ from the above equation, we have:
\begin{gather}
    E\left\{[(-1)(X-E(X|Y,Z)] + X-E(E(X|Y,Z)|Y)]h(Y)\right\}=0 \nonumber \\
    \Leftrightarrow (-1)E[(X-E(X|Y,Z))h(Y)] + E[(X-E(E(X|Y,Z)|Y))h(Y)] = 0 \nonumber \\
    \Leftrightarrow E[(X-E(E(X|Y,Z)|Y))h(Y)] = 0
\end{gather}
Where the first equivalence is by the linearity of expectation, and the second is from equation (1).
Consider the orthogonality condition of \(E(X|Y)\): \(E[(X-E(X|Y)h(Y)] = 0\).  This and (2) are both true if and only if \(E(X|Y) = E(E(X|Y,Z)|Y)\).
Therefore, \(E(X|Y) = E(E(X|Y,Z)|Y)\) as desired.
\subsection{1.c}
\begin{enumerate}
    \item \(E[|X_n|]=\mathcal{O(a_n)}\) \\
    This is equivalent to: 
    \[
    \limsup_{n\rightarrow\infty} \frac{E[|X_n|]}{a_n} \leq C
    \]
    for some constant \(C\).  Consider \(P(\frac{E[|X_n|]}{a_n}> \delta)\).  By Markov's Inequality:
    \begin{align*}
        P(\frac{|X_n|}{a_n}> \delta) \leq \frac{E\left [\frac{|X_n|}{a_n}\right ]}{\delta} \\
        =\delta^{-1}E\left [\frac{|X_n|}{a_n}\right ] = \delta^{-1}\frac{E[|X_n|]}{a_n}
    \end{align*}
    because \(a_n\) is a series of non-stochastic, non-negative numbers. \\
    Because \(E[|X_n|]=\mathcal{O(a_n)}\), for a $n$ large enough, that is \(n\geq n_0 \in {\Bbb N}\):
    \[
    \delta^{-1}\frac{E[|X_n|]}{a_n} \leq \frac{C}{\delta}
    \]
    So, for every \(\varepsilon>0\), chose $\delta$ such that \(\frac{C}{\delta}<\varepsilon\).  We now have that for every \(\varepsilon>0, \exists \;n\geq n_0 \in {\Bbb N}\;\&\; \delta>0 \) such that:
    \[
    P \left (\frac{X_n}{a_n} > \delta \right) \leq P \left (\frac{|X_n|}{a_n}> \delta\right ) <\varepsilon
    \]
    \(\therefore X_n =\mathcal{O}_p(a_n)\) as desired.
    \item \(E[X_n^2]=\mathcal{O}{(b_n)}\) \\
    This is equivalent to: 
    \begin{align*}
        \limsup_{n\rightarrow\infty} \frac{E[X_n^2]}{b_n} \leq C  \nonumber\\
        \Leftrightarrow \quad \limsup_{n\rightarrow\infty} E[X^2] \leq C \cdot b_n \nonumber \\
        \Leftrightarrow \quad \limsup_{n\rightarrow\infty} (E[X^2])^{1/2} \leq \sqrt{C \cdot b_n} 
    \end{align*}
    \(f(x) = x^2\) is a concave function, so by Jensen's inequality:
    \[
    (E[X_n^2])^{1/2} = (E[|X_n|^2])^{1/2} \geq E[(|X_n|^2)^{1/2}] = E[|X_n|]
    \]
    Note that \(C' = \sqrt{C}\) is still a constant.  We have:
    \begin{align*}
         \limsup_{n\rightarrow\infty}E[|X_n|] \leq \limsup_{n\rightarrow\infty} (E[X^2])^{1/2} \leq C'b^{1/2} \\
         \Leftrightarrow \limsup_{n\rightarrow\infty} \frac{E[|X_n|]}{b_n^{1/2}} \leq C' \\
         \Leftrightarrow E(|X_n|) = \mathcal{O}(b_n^{1/2})
    \end{align*}
    By part 1, and because \(b_n^{1/2}\) is a series of non-negative, non-stochastic numbers, \(X_n=\mathcal{O}_p(b_n^{1/2})\)
\end{enumerate}